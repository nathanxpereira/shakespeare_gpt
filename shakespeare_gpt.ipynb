{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.path.join(os.getcwd()) not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "    \n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import tiktoken\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from gpt import GPT\n",
    "import transformers\n",
    "from utils.DataProcessing import DataProcessing\n",
    "from utils.ShakespeareDataset import ShakespeareDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Tiny Shakespeare dataset\n",
    "file_dir =\"data\"\n",
    "text_file = os.path.join(file_dir, 'mini_shakespeare.txt')\n",
    "data_dir = os.path.join(file_dir, 'mini_shakespeare_datasets')\n",
    "tokenized_dir = os.path.join(file_dir, 'tokenized_mini_shakespeare_datasets')\n",
    "\n",
    "batch_size = 64\n",
    "context_len = 128\n",
    "\n",
    "# 90% train, 10% test + validation\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained('./deepseek_tokenizer/', trust_remote_code=True)\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "train_val_split = 0.9\n",
    "dataset_generator = DataProcessing(batch_size=batch_size, block_size=context_len)\n",
    "data = dataset_generator.generate_dataset(text_file, data_dir, split=train_val_split, tokenizer=tokenizer)\n",
    "\n",
    "dataloaders = {key: ShakespeareDataset(data[key], batch_size=batch_size, block_size=context_len) for key in data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(dataloaders['train']))[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32    # embed size\n",
    "heads = 8           # heads for attention\n",
    "num_layers = 3      # number of transformer layers\n",
    "max_length = 128    # max length input vector for postional embedding\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = tokenizer.n_vocab # for word embedding. Mapping dictionary of size N to embed size\n",
    "trg_vocab_size = tokenizer.n_vocab # for word embedding. Mapping dictionary of size N to embed size\n",
    "model_dir = 'models/test'\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "model = GPT(context_len=context_len,\n",
    "            vocab_size=tokenizer.n_vocab,\n",
    "            embed_dim=embed_dim,\n",
    "            heads=heads,\n",
    "            num_layers=num_layers, \n",
    "            device=device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3299617\n"
     ]
    }
   ],
   "source": [
    "num_params = 0\n",
    "for layer in model.parameters():\n",
    "    num_params += layer.numel()\n",
    "        \n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "num_epochs = 5\n",
    "lr = 1e-4\n",
    "print_freq = 100\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss = 0\n",
    "    for idx, data in tqdm(enumerate(dataloaders['train']), total = len(dataloaders['train'])):\n",
    "        optimizer.zero_grad()\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        logits = model.forward(x=x)\n",
    "        loss = model.calculate_loss(logits=logits, targets=y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if np.mod(idx+1, print_freq)==0:\n",
    "            print(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss: {running_loss/len(dataloaders['train'])}\")\n",
    "    \n",
    "    torch.save(model, os.path.join(model_dir, f\"gpt2_epoch{epoch}.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natha\\AppData\\Local\\Temp\\ipykernel_8736\\3275474236.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('models\\\\test\\\\gpt2_epoch4.pth')\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('models\\\\test\\\\gpt2_epoch4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400, 280, 286, 198, 1001, 198, 5896, 198, 198, 198, 198, 7351, 198, 887, 198, 314, 198, 198, 198, 198, 705, 198, 198, 198, 198, 198, 198, 314, 10846, 314, 198, 1867, 198, 1521, 616, 644, 198, 198, 644, 198, 198, 198, 198, 198, 749, 198, 198, 15967, 198, 644, 198, 438, 198, 198, 314, 198, 314, 705, 198, 198, 314, 198, 198, 326, 314, 198, 198, 198, 35205, 198, 198, 644, 198, 198, 11738, 314, 198, 198, 1793, 14210, 7911, 14210, 2940, 1867, 198, 317, 198, 198, 7361, 314, 314, 198, 198, 198, 198, 198, 705, 198, 198, 198, 314, 198, 894, 326, 198, 810, 314, 314, 198, 1338, 355, 783, 198, 198, 314, 1867, 314, 616, 10889, 616, 894, 611, 198, 198, 705, 705, 703, 314, 481, 407, 14186, 354, 406, 1503, 32043, 198, 10462, 451, 645, 2300, 11, 290, 606, 6487, 11, 329, 11, 1497, 13, 198, 2514, 787, 1242, 0, 7271, 922, 640, 286, 345, 1276, 19059, 465, 3251, 588, 284, 4123, 262, 19383, 284, 517, 198, 19626, 276, 11, 1690, 1745, 307, 257, 25920, 4249, 281, 17865, 2402, 262, 2116, 11, 326, 198, 1199, 577, 257, 5898, 198, 26677, 4197, 286, 11906, 2456, 11, 198, 198, 40, 1612, 13, 198, 8491, 4329, 198, 40, 2937, 25, 198, 69, 21823, 783, 674, 34548, 13, 198, 198, 1135, 1276, 550, 301, 407, 11, 262, 1334, 11, 5448, 286, 883, 12839, 307, 523, 4437, 422, 340, 284, 595, 5891, 11, 1012, 34656, 198, 1870, 256, 24275, 416, 511, 43844, 1175, 2339, 4473, 30, 198, 198, 2061, 288, 4649, 2992, 351, 1918, 481, 340, 4445, 19338, 4020, 16746, 318, 314, 766, 502, 407, 0, 887, 11, 561, 25, 198, 2949, 29770, 945, 11, 198, 5167, 621, 329, 198, 8491, 530, 11, 351, 257, 8046, 25, 198, 480, 11, 2258, 4494, 0, 644, 389, 475, 407, 284, 30, 1318, 198, 15266, 284, 466, 1975, 262, 15268, 286, 1175, 318, 1711, 286, 326, 2642, 11, 198, 25492, 65, 2501, 338, 23887, 587, 438, 4053, 11, 645, 47524, 947, 416, 262, 3280, 267, 6, 17034, 262, 2300, 13, 198, 1870, 1011, 198, 2061, 1176, 503, 286, 345, 743, 345, 13279, 625, 3609, 11, 15967, 11, 994, 13, 198, 35, 52, 7336, 25, 198, 1662, 23402, 393, 2048, 922, 7711, 2637, 198, 198, 9858, 1268, 40, 1239, 379, 3991, 402, 2951, 326, 345, 11, 512, 22304, 25, 198, 5832, 2810, 1865, 5637, 1363, 13, 198, 198, 198, 35510, 37, 7112, 1503, 9131, 4261, 18310, 25, 6708, 11, 438, 198, 198, 39, 776, 621, 257, 308, 26872, 262, 661, 338, 43210, 4167, 338, 262, 661, 389, 3148, 481, 307, 1573, 25, 198, 1532, 314, 423, 588, 572, 262, 11083, 286, 683, 13, 198, 34, 56, 25, 198, 44879, 40, 1010, 645, 8716, 49697, 25, 198, 44631, 198, 198, 2484, 439, 11, 379, 567, 31984, 11, 198, 1870, 356, 5708, 11, 33200, 11, 503, 26, 198, 198, 33, 1677, 44558, 5883, 13246, 28182, 25, 198, 45, 9795, 6711, 25, 484, 475, 351, 534, 3772, 10785, 2411, 26, 198, 8086, 25, 198, 35653, 1165, 0, 5658, 1158, 6613, 3211, 1549, 262, 5822, 286, 8433, 4015, 11, 339, 11, 198, 33177, 24212, 51, 25, 198, 2437, 288, 1546, 5781, 25, 198, 1870, 7039, 65, 4548, 475, 616, 14442, 4958, 26, 314, 517, 286, 10598, 30, 198, 37, 7112, 1503, 2767, 25, 198, 464, 1064, 616, 845, 15581, 2988, 11, 15849, 0, 3991, 6016, 11, 198, 817, 88, 2877, 734, 18985, 481, 534, 1029, 6178, 21093, 284, 616, 3956, 26, 198, 1532, 314, 1183, 307, 6580, 3447, 1549, 263, 326, 3280, 28112, 6, 263, 284, 1826, 2642, 11, 5575, 2064, 338, 83, 281, 10758, 13, 198, 198, 3237, 333, 22379, 11, 14210, 1242, 257, 3996, 765, 12, 6286, 20218, 500, 393, 26640, 198, 1870, 287, 15744, 6413, 198, 464, 1334, 11, 198, 1870, 588, 257, 582, 338, 9292, 198, 40, 890, 198, 1870, 16498, 5894, 297, 396, 286, 6164, 11, 423, 616, 10195, 11, 2270, 477, 517, 7213, 351, 3650, 198, 12081, 11, 475, 15581, 22427, 30, 198, 1544, 373, 407, 314, 307, 14748, 11, 314, 2314, 772, 612, 3848, 351, 44823, 11, 14210, 42900, 11401, 56, 14670, 25, 329, 607, 27081, 25, 1309, 514, 772, 198, 1544, 546, 534, 42269, 0, 198, 2514, 40908, 481, 910, 13, 198, 40, 716, 645, 33378, 26, 198, 40, 329, 257, 4601, 340, 407, 284, 1683, 477, 20905, 198, 6090, 5879, 11, 16144, 3032, 345, 1577, 683, 616, 49192, 286, 1854, 12, 28177, 680, 1549, 257, 1310, 407, 922, 379, 345, 26, 466, 340, 339, 7428, 286, 10598, 11, 262, 29654, 286, 607, 319, 2910, 284, 683, 11, 198, 22366, 284, 466, 878, 198, 10248, 1460, 11, 3025, 8245, 11, 198, 15597, 10732, 318, 884, 2839, 2779, 8616, 11, 198, 198, 1870, 883, 198, 7120, 48565, 13, 198, 198, 28042, 356, 11, 286, 1971, 25, 198, 5122, 16336, 13, 198, 198, 9858, 1268, 40, 1183, 1667, 36, 25, 198, 40, 2937, 25, 1474, 13, 198, 8642, 558, 705, 48010, 1683, 198, 45472, 10426, 1677, 40, 2767, 8363, 25, 198, 64, 1755, 318, 407, 481, 290, 477, 6164, 11, 617, 3297, 314, 4444, 4145, 1545, 13, 198, 198, 198, 2633, 257, 3840, 13, 198, 41, 6239, 566, 26, 198, 1212, 1200, 26, 4236, 31277, 1028, 674, 26322, 12, 13424, 6050, 11, 284, 534, 19343, 318, 465, 551, 1442, 2045, 546, 11906, 5465, 198, 5195, 25, 198, 39, 1677, 18276, 347, 3535, 1565, 9399, 25, 198, 17784, 9538, 30, 198, 817, 88, 1021, 290, 880, 326, 339, 1276, 423, 1760, 257, 922, 481, 28145, 198, 817, 272, 7077, 787, 326, 339, 7363, 606, 25, 314, 2236, 22127, 994, 11, 307, 2666, 26, 198, 13482, 3339, 461, 514, 555, 7784, 13, 198, 818, 5968, 13, 198, 8241, 743, 314, 1745, 705, 6511, 1549, 262, 1334, 338, 407, 407, 766, 11, 329, 534, 1664, 30, 198, 198, 464, 25590, 286, 1918, 11, 674, 2000, 278, 286, 477, 466, 2236, 25073, 6937, 282, 13, 198, 6, 12445, 7074, 26, 198, 40569, 16868, 32, 10319, 11, 1123, 19354, 13]\n",
      "thou of\n",
      " Se\n",
      " Love\n",
      "\n",
      "\n",
      "\n",
      " serving\n",
      " But\n",
      " I\n",
      "\n",
      "\n",
      "\n",
      " '\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " I lady I\n",
      " What\n",
      " why my what\n",
      "\n",
      " what\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " most\n",
      "\n",
      " sir\n",
      " what\n",
      "--\n",
      "\n",
      " I\n",
      " I '\n",
      "\n",
      " I\n",
      "\n",
      " that I\n",
      "\n",
      "\n",
      " Fare\n",
      "\n",
      " what\n",
      "\n",
      " Rat I\n",
      "\n",
      " God thou Come thou Mark What\n",
      " A\n",
      "\n",
      " Sir I I\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " '\n",
      "\n",
      "\n",
      " I\n",
      " bel that\n",
      " where I I\n",
      " Sp as now\n",
      "\n",
      " I What I my Sure my bel if\n",
      "\n",
      " ' ' how I will not preciousch LAR PET\n",
      "Swear no matter, and them laugh, for, away.\n",
      "To make art! publicly good time of you must owe his arrest like to beg the Fifth to more\n",
      "Considered, often hold be a maid nor an oath upon the self, that\n",
      "Whose afoot\n",
      "Cry fit of thy words,\n",
      "\n",
      "I mean.\n",
      "Are becomes\n",
      "IUS:\n",
      "fouring now our senate.\n",
      "\n",
      "We must hadst not, the rest, healthy of those thereby be so spirit from it to dis fellow, Clowder\n",
      "And tinker by their deities warlike trial?\n",
      "\n",
      "What duke profess with death will it daily hast mistresses is I see me not! But, would:\n",
      "No tidars,\n",
      "More than for\n",
      "Are one, with a fault:\n",
      "ame, Northumber! what are but not to? There\n",
      "written to do believe the shapes of war is hour of that wrong,\n",
      "Tybalt's kindness been--well, no rogues by the answer o'rt the matter.\n",
      "And take\n",
      "What power out of you may you shake overae, sir, here.\n",
      "DUKE:\n",
      "not incapable or almost good uncle.'\n",
      "\n",
      "COMINI never at false G eyes that you, adieu:\n",
      "you provided yet remaining home.\n",
      "\n",
      "\n",
      "NORFRIAR LAURENCE: Aff,--\n",
      "\n",
      "Hath than a gadding the people's gracious peace's the people are fair will be word:\n",
      "If I have like off the Duke of him.\n",
      "CY:\n",
      "CORIters no joyORGE:\n",
      "Rail\n",
      "\n",
      "Shall, atere lodged,\n",
      "And weola, lords, out;\n",
      "\n",
      "BENVOLUMBERLAND:\n",
      "NARD III: they but with your happy drunkrel;\n",
      "Att:\n",
      "Were too! Thomasves proud arm'd the king of tribunes, he,\n",
      "CAPULET:\n",
      "How dESTER:\n",
      "And Tybait but my loving master; I more of Rome?\n",
      "FRIARET:\n",
      "The find my very noble father, nurse! false bright,\n",
      "Thy living two metres will your high admiral to my brother;\n",
      "If I'll be afresh'der that answer!--'er to meet wrong, Montague'st an bold.\n",
      "\n",
      "Allurril, thou art a bed want-born tempine or Joan\n",
      "And in Padua\n",
      "The rest,\n",
      "And like a man's fallen\n",
      "I long\n",
      "And dare dryllist of mine, have my shame, break all moreeping with store\n",
      "never, but noble gentleman?\n",
      "He was not I be pursuit, I cannot even there calls with parting, thou lovPEYORK: for her temples: let us even\n",
      "He about your hideous!\n",
      "To yawn will say.\n",
      "I am no torment;\n",
      "I for a wish it not to ever all specially\n",
      "Can prove, alias hon you give him my trespass of others-almostish'd a little not good at you; do it he drawn of Rome, the knot of her on blood to him,\n",
      "nothing to do before\n",
      "Goodides, whose stable,\n",
      "Keep carries is such private base Henry,\n",
      "\n",
      "And those\n",
      "Your majesty.\n",
      "\n",
      "Unless we, of York:\n",
      "Our patience.\n",
      "\n",
      "COMINI'll marE:\n",
      "IUS: near.\n",
      "Grace 'tis ever\n",
      "POLIXENIET IV:\n",
      "a night is not will and all mine, some sort I ended thus friend.\n",
      "\n",
      "\n",
      "joy a reasons.\n",
      "JULook;\n",
      "This child; agree attendant against our solemn-black suit, to your lamb is his envious looking about thy hate\n",
      "Why:\n",
      "HENRY BOLANIO:\n",
      "Care heaven?\n",
      "Thy hand and well that he must have done a good will pent\n",
      "Than duty make that he lies them: I shall confess here, be leave;\n",
      "God pleak us unbound.\n",
      "In hell.\n",
      "Who may I hold 'long'd the rest's not not see, for your company?\n",
      "\n",
      "The purity of death, our minding of all do shall withstand constantal.\n",
      "' torn exceed;\n",
      "MARIANA gross, each jealous.\n"
     ]
    }
   ],
   "source": [
    "model.generate(tokenizer=tokenizer, context='thou', generate_len=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into hydra, wandb, and lightning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
