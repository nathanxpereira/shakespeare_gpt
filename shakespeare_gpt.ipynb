{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.path.join(os.getcwd(), '../DeepLearningToolbox') not in sys.path:\n",
    "    sys.path.append(os.path.join(os.getcwd(), '../DeepLearningToolbox'))\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import dataset_handling\n",
    "\n",
    "from DeepLearningModules import transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 40001/40001 [00:00<00:00, 149039.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 29499/29499 [00:00<00:00, 159497.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1639/1639 [00:00<00:00, 72457.36 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1639/1639 [00:00<00:00, 67877.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Download the Tiny Shakespeare dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "file_dir =\"data\"\n",
    "text_file = os.path.join(file_dir, 'mini_shakespeare.txt')\n",
    "data_dir = os.path.join(file_dir, 'mini_shakespeare_datasets')\n",
    "\n",
    "# 90% train, 10% test + validation\n",
    "dataset_handling.generate_dataset(text_file, data_dir, url=url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 29499\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1639\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1639\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_from_disk(\"data/mini_shakespeare_datasets\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_dataset = {}\n",
    "for key in dataset:\n",
    "    tokenized_dataset[key] = dataset_handling.process_data(dataset[key], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   40,   481,   351,  ..., 50256, 50256, 50256],\n",
      "        [   56, 14670,    25,  ..., 50256, 50256, 50256],\n",
      "        [  464,   886,   286,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [   46,  4334,  1661,  ..., 50256, 50256, 50256],\n",
      "        [ 2514,  8616, 10797,  ..., 50256, 50256, 50256],\n",
      "        [    6, 10915,   477,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "dataloaders = {key: DataLoader(tokenized_dataset[key], batch_size=batch_size) for key in tokenized_dataset.keys()}\n",
    "print(next(iter(dataloaders['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = transformer.Transformer(embed_size, heads, num_layers, max_length, vocab_length, dropout=0.1, device='cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
